{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from sklearn import metrics\n",
        "import pandas as pd\n",
        "\n",
        "results = pd.read_csv(Path(\"./results/results.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "def metrics_to_df(dataset: pd.DataFrame, classifier_name: str, y_pred_label: str):\n",
        "    y_true = dataset[\"is_gpt\"]\n",
        "    y_predicted = dataset[y_pred_label]\n",
        "\n",
        "    df = pd.DataFrame(\n",
        "        columns=[\n",
        "            \"tn\",\n",
        "            \"fp\",\n",
        "            \"fn\",\n",
        "            \"tp\",\n",
        "            \"precision\",\n",
        "            \"recall\",\n",
        "            \"f1-score\",\n",
        "            \"accuracy\",\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    confusion_matrix = list(\n",
        "        metrics.confusion_matrix(y_true=y_true, y_pred=y_predicted).ravel()\n",
        "    )  # [\"tn\", \"fp\", \"fn\", \"tp\"]\n",
        "\n",
        "    precision_recall_fscore_support = list(\n",
        "        metrics.precision_recall_fscore_support(\n",
        "            y_true=y_true, y_pred=y_predicted, average=\"binary\"\n",
        "        )[:-1]\n",
        "    ) # precision, recall, f1-score\n",
        "    \n",
        "    accuracy = metrics.accuracy_score(y_true=y_true, y_pred=y_predicted)\n",
        "\n",
        "    row = confusion_matrix + precision_recall_fscore_support + [accuracy]\n",
        "\n",
        "    df.loc[classifier_name] = row\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def metrics_to_csv(dataset: pd.DataFrame):\n",
        "    # create metrics dataframe\n",
        "\n",
        "    gptzero_metrics = metrics_to_df(dataset, \"gptzero\", \"gptzero_score_overall\")\n",
        "    orginalityai_metrics = metrics_to_df(dataset, \"originalityai\", \"originality_score_overall\")\n",
        "\n",
        "    csv = pd.concat([gptzero_metrics, orginalityai_metrics])\n",
        "    csv.to_csv('./results/metrics.csv')\n",
        "\n",
        "    return csv\n",
        "\n",
        "classifer_metrics = metrics_to_csv(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
